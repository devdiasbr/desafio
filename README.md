# Desafio de Engenharia de Dados - Pipeline de Vendas

Este projeto implementa um pipeline de dados de vendas completo seguindo a arquitetura MedalhÃ£o (Bronze, Silver, Gold), utilizando PySpark e Delta Lake. O projeto foi estruturado de forma modular e suporta **execuÃ§Ã£o hÃ­brida** (Local em Windows/Linux e Databricks).

## ğŸ“‹ DescriÃ§Ã£o do Projeto

O objetivo Ã© processar dados de vendas de forma incremental, garantindo qualidade, deduplicaÃ§Ã£o e agregaÃ§Ãµes para anÃ¡lise.

### Arquitetura MedalhÃ£o

1.  **Bronze (IngestÃ£o):**
    *   LÃª arquivos CSV brutos da pasta `dados_vendas`.
    *   Processamento incremental: lÃª apenas arquivos novos que ainda nÃ£o foram processados.
    *   Adiciona metadados: `data_carga` (timestamp) e `nome_arquivo` (origem).
    *   Armazena os dados brutos em formato Delta.

2.  **Silver (Qualidade e DeduplicaÃ§Ã£o):**
    *   LÃª dados da camada Bronze.
    *   Remove duplicatas utilizando `Window functions` baseadas no `codigo_venda`, priorizando o registro mais recente.
    *   Utiliza operaÃ§Ã£o `MERGE` (SCD Tipo 1) para atualizar registros existentes ou inserir novos na tabela Delta.

3.  **Gold (Modelagem e AgregaÃ§Ã£o):**
    *   **Fato Vendas:** Tabela detalhada e limpa pronta para anÃ¡lise. Utiliza `MERGE` para manter consistÃªncia.
    *   **Vendas Agregadas:** Tabela sumarizada com o valor total de vendas agrupado por `Produto`, `Ano` e `MÃªs`. TambÃ©m atualizada via `MERGE`.

## ğŸ› ï¸ Tecnologias Utilizadas

*   **Python 3.x**
*   **PySpark**: Processamento distribuÃ­do de dados.
*   **Delta Lake**: Camada de armazenamento que traz confiabilidade (ACID) para Data Lakes.
*   **Databricks**: Plataforma unificada de anÃ¡lise de dados (compatÃ­vel).
*   **Hadoop (Winutils)**: BinÃ¡rios necessÃ¡rios para rodar Spark no Windows (apenas local).
*   **Unittest**: Framework de testes automatizados.

## ğŸš€ Como Executar

### OpÃ§Ã£o 1: ExecuÃ§Ã£o Local (Windows/Linux)

#### PrÃ©-requisitos
1.  Python instalado (versÃ£o 3.8 ou superior recomendada).
2.  Java instalado (JRE/JDK 8 ou 11) e configurado no `JAVA_HOME`.

#### InstalaÃ§Ã£o
1.  Clone este repositÃ³rio.
2.  Instale as dependÃªncias:
    ```bash
    pip install -r requirements.txt
    ```

#### Executando o Pipeline
Para rodar o pipeline completo:
```bash
python main.py
```

Para rodar limpando dados anteriores (reset):
```bash
python main.py --clean
```

### OpÃ§Ã£o 2: ExecuÃ§Ã£o no Databricks

1.  **Databricks Repos:**
    *   No workspace do Databricks, vÃ¡ em "Repos" e clone este repositÃ³rio.
    *   Certifique-se de que o cluster tenha suporte a PySpark e Delta Lake (padrÃ£o no Databricks Runtime).

2.  **ExecuÃ§Ã£o:**
    *   VocÃª pode abrir o arquivo `main.py` e clicar em "Run".
    *   Ou criar um **Job** apontando para o arquivo `main.py` no Repo.
    *   Ou importar os mÃ³dulos em um Notebook:
        ```python
        from main import main
        main()
        ```
    *   *Nota:* O cÃ³digo detectarÃ¡ automaticamente o ambiente e usarÃ¡ a `SparkSession` do cluster.

### Executando os Testes

Para validar a lÃ³gica (funciona em ambos os ambientes):

```bash
python -m unittest tests/test_pipeline.py
```

## ğŸ“‚ Estrutura do Projeto

```text
.
â”œâ”€â”€ dados_vendas/          # Arquivos CSV de entrada (Raw Data)
â”œâ”€â”€ data/                  # DiretÃ³rio de saÃ­da dos dados processados (Delta Tables)
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ hadoop/                # BinÃ¡rios do Hadoop para Windows (winutils.exe)
â”œâ”€â”€ src/                   # CÃ³digo fonte do pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bronze.py          # LÃ³gica da camada Bronze
â”‚   â”œâ”€â”€ silver.py          # LÃ³gica da camada Silver
â”‚   â”œâ”€â”€ gold.py            # LÃ³gica da camada Gold
â”‚   â””â”€â”€ utils.py           # ConfiguraÃ§Ãµes do Spark (HÃ­brido) e utilitÃ¡rios
â”œâ”€â”€ tests/                 # Testes automatizados
â”‚   â””â”€â”€ test_pipeline.py   # Testes de integraÃ§Ã£o do pipeline
â”œâ”€â”€ main.py                # Orquestrador principal (Entry Point)
â”œâ”€â”€ requirements.txt       # DependÃªncias do projeto
â””â”€â”€ README.md              # DocumentaÃ§Ã£o
```

## âš™ï¸ Detalhes de ImplementaÃ§Ã£o

*   **Suporte HÃ­brido:** O mÃ³dulo `src/utils.py` detecta se o cÃ³digo estÃ¡ rodando localmente ou no Databricks (`DATABRICKS_RUNTIME_VERSION`) e ajusta as configuraÃ§Ãµes automaticamente.
*   **Modularidade:** O cÃ³digo foi refatorado para o diretÃ³rio `src/`, separando responsabilidades.
*   **Controle Incremental:** Na camada Bronze, o cÃ³digo verifica diretamente os arquivos jÃ¡ existentes na tabela Delta para evitar reprocessamento (idempotÃªncia sem arquivos de controle externos).
*   **Armazenamento HÃ­brido:** 
    *   **Local (Windows/Linux):** Utiliza o diretÃ³rio `data/` para armazenar as tabelas Delta.
    *   **Databricks:** Utiliza **Tabelas Gerenciadas** dentro de um Database especÃ­fico (`desafio_beca`), garantindo isolamento e organizaÃ§Ã£o no Hive Metastore / Unity Catalog.
*   **IdempotÃªncia:** As camadas Silver e Gold utilizam `MERGE` para garantir consistÃªncia.

## ğŸ“Š Schema dos Dados

**Bronze & Silver:**
*   `codigo_venda` (String): Identificador Ãºnico.
*   `numero_fiscal` (Integer)
*   `id_produto` (Integer)
*   `nome_produto` (String)
*   `valor` (Double)
*   `timestamp_venda` (Timestamp)
*   `data_carga` (Timestamp)
*   `nome_arquivo` (String)

**Gold (Agregada):**
*   `nome_produto` (String)
*   `ano` (Integer)
*   `mes` (Integer)
*   `valor_total` (Double)
